* Bat
- 64% at 1:36
- 56% at 1:55 = 20 min for 8%
  24% per hour
  - 2 hours
  - last till 4.
    


* TODAY
- [ ] prototype enums
- [ ] prototype list editing (up, down, del etc)
- [ ] integrate proto stuff back into generated
- [ ] fix row id generation (if needed)
- [ ] get click on list to do load
- [ ] get page load to work
- [ ] get page save to work

- think about multiple pages in one HTML page - including lists.
- think about the larger wrapper for entity (for list editing, queries,
  invariants etc).
- fix alignment of menu button

* Next - Enums!
- enum (join) figure out vue model.
- can do a slick job, with seach filtering etc.
- can lazy load list (for the big joins - like lex list)
- is possible to update lists - but leave as a later feature.
- use this to make all? enums driven from DB tables.
- as things go is not too bad that user has to save, add category, then go back to word.
- define some of these tables in importer and push stuff in.

* Next - lists
- default behaviour is one empty item - with next item added.
- any entry that is completetly cleared out is removed.
* Next - swith public note to HTML or text area

* Next table editor as a unit
- specify a bit more, and should get a complete table editor (browse list with query etc,
  insert/delete/edit functionality etc.
- probably a class that we override to get this behaviour.


* Tables we need
- entry
- pacific page
- pacific import unit

  
- category
- user
  - edit permissions
  - learning info
- language variant

- home page
- story page
- ...

* RRBR version
- pages
- event
  - time
  - location id
  - volunteer ids
  - cash raised
  - bikes fixed (at least postal codes)
- location (may have directions, picture etc)
- 

* Model changes
- make definition be a list.
- last change rethink.  (probably keeping history etc).
- 'Selector' is confusing 'Locale'  Language Variant
- make enums work.
- enums should probably be populated from tables (need for things like categories anyway).
- text areas should be markdown? or similar.





* TODO
- make word compilation to html be a lazy computed item (not saved to disk)
- make form styling reasonable
- make wordlist click load a word in form
- make save update word.
- move table rendreing specification into the model.
- make model for more things.
- 





* Things that maybe shoud have selector
- status
- definitions - change to list
- examples?  (covered by text)
- glosses (covered by text)
- other regional forms
- attrs.

Selectors will be pulldown + custom if chosen.

* TODO
- do a very nice entry render, then play with editing


* TODO
- [ ] switch wordlist to use tableformatter.
- [ ] update model to correspond to model updates.
- [ ] fiddle with rendering of model.
- [ ] make model load/save from DB.
- [ ] make codegen happen at startup.

- still have more modelling changes, but priority for rest of day
  is editing what we have.
- queries should be done in-core.
- should be able to request views of DB (based on scope system).

* Scope flattening.
- mm mm-li  mm-sf  mm-sf-cb  mm-sf - mm.cb
- is output a single context?
- exprs can b powerful, but if final result is a fixed number
  of defined contexts (which can be defined in code), then
  can denormalize as we define new contexts.

* DM questions
- RAND images?
- you are putting processing multiople dict entries at once - much harder!
* References are subset of unit image
- dianne has not seen more than one.
* Transliteration - may have more later
- expect their might be
- need to have multiple orthos
* Result
- Resolved Reference
* How to do scoping
- status is scoped.
- li, sf, ??, both, all but
- can have columns for resolved scopes + a textual expr that decomposes
  to cols.
* Rand source format
- send me whatever you have
* Review with researchers
- date, text.
- published ???
* HOW HARD TO switch to SQLite?
- key is model.
- rowid is 64 bit.
- finish cleanup model first, then try!
- we do have 3 level (li/sf inside an example for example) - so
  will have more complciated join setup.
* TODO
- [ ] update model.
- [ ] write validation fn for model, and run against import
- [ ] update code gen for model
- [ ] write python fn to find next local id.
- [ ] write js function to find next local id.
- [ ] remodel related entries.


- once have versioning, then lexeme does not have lex label - just
  versioning.

- 
  
* Should li/sf be entity list?
- probably?
- can also have multiple spellings for one lex.
- spellings can have comments.
- local id space is unique - so these will flatten nicely.

* Scope
- each separate assertion (thing with an id) should have a scope expr
- this is versions of dict it is active for.
- want to model so is not to hard to use in SQL.  

* TODO
- [ ] setup server for banq to upload to
- [ ] send banq email
- [X] change lexeme id to be int
- [ ] figure out id mechanism for lists, and implement
  - want ids to be local to entity (so not too huge)
  - probably 3 digit, starting at 100.
  - if not hard, just make scan entry, collecting all ints etc.
  - or have a local allocator?
  - can init once, then use repeatedly.
  - field is called 'id' (separate from top level _id)
  - no reuse - just find highest.
- [ ] make nice rendering of entry
- [ ] 


* Page data size
- sample page is 50MB
- vol 1 is 200p
- vol 2 is 233p
- vol 3 is 202p
- vol 4 is 194p

- so 200pages per volume * 4 = 800pages
- round up to 1000pages = 50MB * 1000 = 50GB total data.


* Model work
- make list viewer.


* List viewer
- naked (non vue) table.
- row per lexeme.
- each row has an id.
- on click, do a AJAX DB call that increments a counter in the record,
  then causes an incremental rerender of the row.


* Tools look fine, bulk convert data so can do final form!


* Virtualenv
- setup virtualenv
- install deps in virtual env + pymongo
* Vue
- get combined vue quasar example working
- update form to be a quasar dialog.q
* Quasar
- play with grid.
* Data conversion
- do hack conversion of data into mongo
- make lexeme edit points.
* Data model
- figure out data requirements for new thing.
  
